{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS4023 Machine Learning : Ensemble Learning Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise, you'll explore different ensemble methods and how does ensemble improves the performance of a machine learning model. There are three parts in this exercise:\n",
    "1. Simple ensemble strategy: majority voting\n",
    "2. Bagging Method\n",
    "3. Boosting Method: Adaboost\n",
    "\n",
    "The dataset we use for this exercise is a cancer dataset with 699 instances and a total number of 9 features labeled in either benign or malignant classes (0 for benign, 1 for malignant). The dataset only contains numeric values and has been normalized.\n",
    "\n",
    "Many methods will use random generator, e.g., train-test split, decision tree model, bagging boostramp sample generation, therefore, we can set the seed to a fixed number in order to achieve same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.379749</td>\n",
       "      <td>0.237164</td>\n",
       "      <td>0.245271</td>\n",
       "      <td>0.200763</td>\n",
       "      <td>0.246225</td>\n",
       "      <td>0.346352</td>\n",
       "      <td>0.270863</td>\n",
       "      <td>0.207439</td>\n",
       "      <td>0.065490</td>\n",
       "      <td>0.344778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.312860</td>\n",
       "      <td>0.339051</td>\n",
       "      <td>0.330213</td>\n",
       "      <td>0.317264</td>\n",
       "      <td>0.246033</td>\n",
       "      <td>0.364071</td>\n",
       "      <td>0.270929</td>\n",
       "      <td>0.339293</td>\n",
       "      <td>0.190564</td>\n",
       "      <td>0.475636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape   \n",
       "count       699.000000               699.000000                699.000000  \\\n",
       "mean          0.379749                 0.237164                  0.245271   \n",
       "std           0.312860                 0.339051                  0.330213   \n",
       "min           0.000000                 0.000000                  0.000000   \n",
       "25%           0.111111                 0.000000                  0.000000   \n",
       "50%           0.333333                 0.000000                  0.000000   \n",
       "75%           0.555556                 0.444444                  0.444444   \n",
       "max           1.000000                 1.000000                  1.000000   \n",
       "\n",
       "       Marginal Adhesion  Single Epithelial Cell Size  Bare Nuclei   \n",
       "count         699.000000                   699.000000   699.000000  \\\n",
       "mean            0.200763                     0.246225     0.346352   \n",
       "std             0.317264                     0.246033     0.364071   \n",
       "min             0.000000                     0.000000     0.000000   \n",
       "25%             0.000000                     0.111111     0.100000   \n",
       "50%             0.000000                     0.111111     0.100000   \n",
       "75%             0.333333                     0.333333     0.500000   \n",
       "max             1.000000                     1.000000     1.000000   \n",
       "\n",
       "       Bland Chromatin  Normal Nucleoli     Mitoses       Class  \n",
       "count       699.000000       699.000000  699.000000  699.000000  \n",
       "mean          0.270863         0.207439    0.065490    0.344778  \n",
       "std           0.270929         0.339293    0.190564    0.475636  \n",
       "min           0.000000         0.000000    0.000000    0.000000  \n",
       "25%           0.111111         0.000000    0.000000    0.000000  \n",
       "50%           0.222222         0.000000    0.000000    0.000000  \n",
       "75%           0.444444         0.333333    0.000000    1.000000  \n",
       "max           1.000000         1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "data = pd.read_csv(\"cancer_normalized.csv\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in to program.\n",
    "x = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Simple Ensemble Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at a simple ensemble technique for classification: majority voting. In this method, multiple models are used to make predictions for each data instance. The predictions by each model are considered as a **vote**. The prediction which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "Scikit-Learn provides us with some handy functions that we can use to accomplish this.\n",
    "- The ``VotingClassifier`` takes in a list of different estimators as arguments and a voting method. The ``hard`` voting method uses the predicted labels and a majority rules system, while the ``soft`` voting method predicts a label based on the sum of the predicted probabilities.\n",
    "\n",
    "Here, we use three models, *Decision Tree*, *SVM* and *LogisticRegression*, for voting and adopt 10-fold cross validation. Report the mean accuracy of **individual classifiers and the ensemble by applying the majority voting strategy (**hard voting**).** Compare the performance. \n",
    "- Note: For DecisionTreeClassifier() implementation, the features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# your implementation here\n",
    "# your implementation here\n",
    "# model for three\n",
    "dt = DecisionTreeClassifier(random_state=seed)\n",
    "lr = LogisticRegression(random_state=seed)\n",
    "svm = SVC(random_state=seed)\n",
    "\n",
    "# voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('dt', dt),\n",
    "    ('lr', lr),\n",
    "    ('svm', svm)\n",
    "    ], voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9485093167701864\n",
      "0.9628571428571429\n",
      "0.9685507246376812\n",
      "0.9642650103519671\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# mean accuarcy for ten fold(represented as cv)\n",
    "dt_accuarcy = cross_val_score(dt,x,y,cv=10).mean()\n",
    "lr_accuarcy = cross_val_score(lr,x,y,cv=10).mean()\n",
    "svm_accuarcy = cross_val_score(svm,x,y,cv=10).mean()\n",
    "vote_accuarcy = cross_val_score(voting_clf,x,y,cv=10).mean()\n",
    "print(dt_accuarcy)\n",
    "print(lr_accuarcy)\n",
    "print(svm_accuarcy)\n",
    "print(vote_accuarcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the bagging method by using decision tree as the base learning algorithm. Scikit-Learn provides us a module of ``BaggingClassifier``, we can provide the base learning model and the number of estimation models. Try to set the number of estimators to 100 and report the mean accuracy of the ensemble using 10-fold cross validation. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "# your implemenation here\n",
    "bag = BaggingClassifier(dt,\n",
    "                        n_estimators = num_trees,\n",
    "                        bootstrap = True,\n",
    "                        n_jobs = -1,\n",
    "                        oob_score = True,\n",
    "                        random_state = seed)\n",
    "bagging_accuarcy = cross_val_score(bag,x,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9557142857142857\n"
     ]
    }
   ],
   "source": [
    "print(bagging_accuarcy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialized a 10-fold cross-validation fold. After that, we instantiated a Decision Tree Classifier with 100 trees and wrapped it in a Bagging-based Ensemble. The accuracy improved to 95.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also provides access to the ``RandomForestClassifier``, which is a modification of the decision tree classification. Use random forest model and report the mean accuracy by using 10-folds cross-validation. Number of trees set to 100.\n",
    "\n",
    "**Compare the performance of RandomForestClassifier with bagged decision tree and give the analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# your implementation here...\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "rf_accuracy = cross_val_score(rf,x,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9671428571428571\n"
     ]
    }
   ],
   "source": [
    "print(rf_accuracy.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison and analysis**:\n",
    "\n",
    "**Random Forest** performs better than **Decision Tree Ensembled using Bagging Method**. \n",
    "\n",
    "1. The randomness of the bagging method only comes from the sample perturbation. And due to attribute perturbation is introduced into the random forest model, so that the generalization performance of the final model can be further improved by increasing the degree of difference between individual learners.\n",
    "\n",
    "2. Random forests are better trained than bagging because each tree in bagging examines all features, while random forests consider only a subset of features.\n",
    "\n",
    "3. Compared with bagging, random forests tend to have poor starting performance at first, but as the number of individual learners increases, random forests converge to smaller errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you use AdaBoost classification by boosting the ``decision stump``(**one-level decision tree**).Try to set the number of rounds to 100 and report the performance of the ensemble. Compare the performance with a single decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/wendywang/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# your implementation here...\n",
    "adaboost = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=1, \n",
    "                                                                        random_state = seed),\n",
    "                                       n_estimators = 100,\n",
    "                                       random_state = seed)\n",
    "adaboost_accuracy = cross_val_score(adaboost,x,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585507246376814\n"
     ]
    }
   ],
   "source": [
    "print(adaboost_accuracy.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
